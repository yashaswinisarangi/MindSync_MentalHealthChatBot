# -*- coding: utf-8 -*-
"""ChatbotInference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K9NCu9z95bcjnt2lAMPzpALUgph3NOas
"""

!pip install --upgrade --force-reinstall "numpy<2.0.0" vllm numba tensorflow torch unsloth

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p /content/outputsss
!unzip "/content/drive/MyDrive/outputs2-20250320T171547Z-001.zip" -d /content/outputsss/

from unsloth import FastLanguageModel, is_bfloat16_supported
from filelock import FileLock
from transformers import AutoTokenizer
import torch
import json
import os
from vllm import SamplingParams

# Model configuration
max_seq_length = 2048
lora_rank = 64

# Load base model with LoRA adapter
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="Qwen/Qwen2.5-3B-Instruct",
    max_seq_length=max_seq_length,
    load_in_4bit=True,
    fast_inference=True,
    max_lora_rank=lora_rank,
    gpu_memory_utilization=0.5,
    lora_weight_path="/content/outputsss/outputsss",  # Correct LoRA path
)

def load_conversations(user_id, history_file="conversations.jsonl", max_history=10):
    """Load conversation history with file locking"""
    conversations = []
    if not isinstance(user_id, (str, int)):
        raise ValueError("Invalid user ID format")

    if os.path.exists(history_file):
        with FileLock(history_file + ".lock"):
            try:
                with open(history_file, "r", encoding="utf-8") as f:
                    for line in f:
                        data = json.loads(line)
                        if str(data["user_id"]) == str(user_id):
                            conversations.append(data)
            except Exception as e:
                print(f"Error loading history: {e}")

    return conversations[-max_history:]

def save_conversation(user_id, user_message, bot_response,
                     history_file="conversations.jsonl", max_history=5):
    """Save conversation with validation and locking"""
    if not isinstance(user_id, (str, int)):
        raise ValueError("Invalid user ID format")

    user_message = user_message.strip()[:2000]  # Limit message length
    bot_response = bot_response.strip()[:2000]

    new_entry = {
        "user_id": str(user_id),
        "user_message": user_message,
        "bot_response": bot_response
    }

    with FileLock(history_file + ".lock"):
        try:
            with open(history_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(new_entry) + "\n")
        except Exception as e:
            print(f"Error saving conversation: {e}")

def generate_response(user_id, user_message, model):
    """Generate response with safety checks"""
    # Input validation
    user_message = user_message.strip()[:2000]
    if not user_message:
        return "Please provide a valid message."

    SYSTEM_PROMPT = """You are a CBT-based mental health chatbot. Follow these rules:
1. Empathize first, then help identify negative thoughts
2. Use Socratic questioning to challenge cognitive distortions
3. Suggest practical behavioral activation strategies
4. Never provide medical advice
5. Maintain natural conversation flow"""

    try:
        past_conversations = load_conversations(user_id)
        chat_history = [{"role": "system", "content": SYSTEM_PROMPT}]

        # Build conversation history
        for conv in past_conversations:
            chat_history.append({"role": "user", "content": conv["user_message"]})
            chat_history.append({"role": "assistant", "content": conv["bot_response"]})

        chat_history.append({"role": "user", "content": user_message})

        # Format input for model
        text = tokenizer.apply_chat_template(
            chat_history,
            tokenize=False,
            add_generation_prompt=True
        )

        # Generation parameters
        sampling_params = SamplingParams(
            temperature=0.85,
            top_p=0.9,
            max_tokens=2048,
            stop=["<|endoftext|>", "USER:", "ASSISTANT:"]
        )

        # Generate response
        outputs = model.fast_generate(
            text,
            sampling_params=sampling_params,
        )
        response = outputs[0].outputs[0].text.strip()

        # Basic safety filter
        response = response.split("ASSISTANT:")[-1].split("USER:")[0].strip()

        # Save conversation
        save_conversation(user_id, user_message, response)
        return response

    except Exception as e:
        print(f"Error generating response: {e}")
        return "I'm having trouble responding right now. Please try again later."

"""CHATBOT INFERENCE"""

def chat_loop():
    user_id = "temp_user"  # You can make this dynamic if needed
    print("\nWelcome to the CBT Chatbot. Type 'exit' to end the conversation.\n")

    while True:
        try:
            # Get user input
            user_input = input("You: ")

            if user_input.lower() in ["exit", "quit"]:
                print("Ending conversation. Take care!")
                break

            # Generate and display response
            response = generate_response(user_id, user_input, model)
            print("\nBot:", response)
            print("---\n")

        except KeyboardInterrupt:
            print("\nConversation ended by user.")
            break
        except Exception as e:
            print(f"\nError: {str(e)}")
            print("Please try rephrasing your message.")

# Start the chat
if __name__ == "__main__":
    chat_loop()











